## 1. Apache Nifi Setup
Import the nifi/nifi_flow.xml file into your Nifi instance.
Configure the GetHBase processor to connect to your HBase instance.
Configure the UpdateAttribute processor to generate a unique filename.
Configure the PutS3Object processor with your AWS S3 credentials and bucket details.
Start the Nifi flow.

## 2. Apache Spark Setup
Configure your Spark environment with the necessary dependencies for HBase and AWS S3.
Update the spark/HBaseToDeltaLake.scala file with your table name, AWS S3 bucket, and any other specific configurations.
Submit the Spark job:
spark-submit --class HBaseToDeltaLake --master your-spark-master spark/HBaseToDeltaLake.scala

spark-submit --class HBaseToDeltaLake --master your-spark-master spark/HBaseToDeltaLake.scala
## 3. Databricks Setup
Ensure Databricks is configured to read from your AWS S3 bucket.
Create a Delta Lake table in Databricks and point it to the S3 location where the Spark job writes the data.
